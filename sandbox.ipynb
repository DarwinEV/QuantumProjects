{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1de84c01-8855-4148-b639-b47b0e02eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e54846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SimpleCustomBatch object at 0x000001B1D2EB2420> 0\n",
      "<__main__.SimpleCustomBatch object at 0x000001B1D2EB2240> 1\n",
      "<__main__.SimpleCustomBatch object at 0x000001B1E6D67440> 2\n",
      "<__main__.SimpleCustomBatch object at 0x000001B1D2EB2420> 3\n",
      "<__main__.SimpleCustomBatch object at 0x000001B1D2EB2240> 4\n"
     ]
    }
   ],
   "source": [
    "class SimpleCustomBatch:\n",
    "    def __init__(self, data):\n",
    "        transposed_data = list(zip(*data))\n",
    "        self.inp = torch.stack(transposed_data[0], 0)\n",
    "        self.tgt = torch.stack(transposed_data[1], 0)\n",
    "\n",
    "    # custom memory pinning method on custom type\n",
    "    def pin_memory(self):\n",
    "        self.inp = self.inp.pin_memory()\n",
    "        self.tgt = self.tgt.pin_memory()\n",
    "        return self\n",
    "\n",
    "def collate_wrapper(batch):\n",
    "    return SimpleCustomBatch(batch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # creates a 10x5 tensor of range(0, 50) incrementing every right column by 1\n",
    "    inps = torch.arange(50, dtype=torch.float32).view(10, 5)\n",
    "    # print(inps)\n",
    "\n",
    "    tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "    # print(tgts)\n",
    "    \n",
    "    dataset = TensorDataset(inps, tgts) # store this in the TensorDataset Class\n",
    "\n",
    "    # STEPS:\n",
    "    # 1: take a tensor and store it\n",
    "    # 2: load the tensor with a batch_size, and other parameters.\n",
    "    loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n",
    "                        pin_memory=True)\n",
    "\n",
    "    # print(help(loader))\n",
    "    \n",
    "    for batch_ndx, sample in enumerate(loader):\n",
    "        print(sample, batch_ndx)\n",
    "        # print(sample.inp.is_pinned())\n",
    "        # print(sample.tgt.is_pinned())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff8d576b-e05f-4de7-9dab-b808028ec535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.7236],\n",
      "        [0.0430]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.8954, 0.5411], requires_grad=True)\n",
      "tensor([[ 1.0322],\n",
      "        [-0.6515]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[1.6422, 0.5856],\n",
      "        [0.4239, 0.5131]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(1, 2)\n",
    "print(m.weight)\n",
    "print(m.bias)\n",
    "\n",
    "input = torch.randn(2, 1)\n",
    "print(input)\n",
    "\n",
    "output = m(input)\n",
    "print(output.size())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a73c9c-f121-4799-a4d0-fc0c7c83b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialTransformer(nn.Module):\n",
    "    def _init_(self, param):\n",
    "        super(FinancialTransformer, self)._init_ # we import the nn.Module methods into the class\n",
    "        self.embedding = nn.Linear(1, 2)\n",
    "        # self.positional_encoder = nn.Embedding(20, )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "037794c5-c069-4cb6-b6cf-ca3ba052032f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 4, 5],\n",
      "        [4, 3, 2, 9]]) torch.Size([2, 4])\n",
      "tensor([[[-0.5620, -0.0164,  0.8274],\n",
      "         [-1.1133, -0.9781,  0.4610],\n",
      "         [ 1.6398,  0.4853,  1.1278],\n",
      "         [ 0.9104, -0.4623,  1.5287]],\n",
      "\n",
      "        [[ 1.6398,  0.4853,  1.1278],\n",
      "         [-0.4503, -1.1623,  0.4724],\n",
      "         [-1.1133, -0.9781,  0.4610],\n",
      "         [-0.1699,  1.8437, -0.1671]]], grad_fn=<EmbeddingBackward0>) torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "print(input, input.size())\n",
    "embed = embedding(input)\n",
    "print(embed, embed.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40eb61-8736-4847-8396-93ec54c643a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
